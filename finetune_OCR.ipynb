{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139f62ca",
   "metadata": {},
   "source": [
    "**Contributed by:**\n",
    "*Rupali K*, \n",
    "*Mahadev S*, \n",
    "*Tendool Srivatsav S*, \n",
    "*Sarang U*, \n",
    "*Roshini A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e4236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tendo\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5add11",
   "metadata": {},
   "source": [
    "*We have a very large data without labels. So, we have to manually create the labels and it is not completed yet. So, we have used a very small part of the data to test the code but we have to create the labels manually and then finetune the model completely and then we will proceed with the deep learning models*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd923b4",
   "metadata": {},
   "source": [
    "*Collection of data took us more time as we need more amount of medical data for this project and they are not completely open sourced, We got the access of the data we need and we are still proceeding with it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73976a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\tendo\\AppData\\Local\\Temp\\ipykernel_32356\\1794566946.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 02:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.097900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, img_folder, labels_file, processor):\n",
    "        self.img_folder = img_folder\n",
    "        self.processor = processor\n",
    "        with open(labels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.texts = [line.strip() for line in f.readlines()]\n",
    "        self.image_files = [f\"{i+1}.jpg\" for i in range(len(self.texts))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_folder, self.image_files[idx])\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\").input_ids.squeeze()\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "model_name = \"microsoft/trocr-base-handwritten\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "\n",
    "\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "\n",
    "\n",
    "\n",
    "img_folder = r\"C:\\Users\\tendo\\OneDrive\\Documents\\Srivatsav\\sem 5\\Big Data\\Project\\archive\\data\"\n",
    "labels_file = \"labels.txt\"\n",
    "\n",
    "\n",
    "if not os.path.exists(labels_file):\n",
    "    generate_pseudo_labels(img_folder, labels_file, model_name)\n",
    "\n",
    "\n",
    "train_dataset = PrescriptionDataset(img_folder, labels_file, processor)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [b[\"labels\"] for b in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # ignore loss on padding\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr-prescription-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,  \n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./trocr-prescription-finetuned\")\n",
    "processor.save_pretrained(\"./trocr-prescription-finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
